# !/usr/bin/env python3
"""
Created on Fri Jun 26 08:37:56 2020

This code is ezBIDS"s attempt to determine BIDS information (DataType,
ModalityLabel, entitiy labels [acq, run, dir, etc]) based on dcm2niix output.
This information is then displayed in the ezBIDS UI, where users can made
edits/modifications as they see fit.

@author: dlevitas
"""

from __future__ import division
import os
import sys
import re
import json
import warnings
from operator import itemgetter
from math import floor
import pandas as pd
import numpy as np
import nibabel as nib
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from PIL import Image
plt.style.use("dark_background")
warnings.filterwarnings("ignore")

# DATA_DIR = sys.argv[1]
DATA_DIR = '/media/data/ezbids/dicoms/OpenScience'
os.chdir(DATA_DIR)

######## Functions ########
def correct_pe(pe_direction, ornt):
    """
    Takes phase encoding direction (pe_direction) and image orientation (ornt)
    to correct pe_direction if need be. This correction occurs if pe_direction
    is in "xyz" format instead of "ijk".

    Function is based on https://github.com/nipreps/fmriprep/issues/2341 and
    original code comes from Chris Markiewicz and Mathias Goncalves.

    Parameters
    ----------
    pe_direction : string
        Value from PhaseEncodingDirection in acquisition json file generated
        by dcm2niix
    ornt: string
        Value of "".join(nib.aff2axcodes(nii_img.affine)), where "nii_img" is
        is the acquisition NIFTI file generated by dcm2niix

    Returns
    -------
    proper_pe_direction: string
        pe_direction, in "ijk" format
    """
    axes = (("R", "L"), ("A", "P"), ("S", "I"))
    proper_ax_idcs = {"i": 0, "j": 1, "k": 2}

    # pe_direction is ijk (no correction necessary)
    if any(x in pe_direction for x in ["i", "i-", "j", "j-", "k", "k"]):
        proper_pe_direction = pe_direction

    # pe_direction xyz (correction required)
    else:
        improper_ax_idcs = {"x": 0, "y": 1, "z": 2}
        axcode = ornt[improper_ax_idcs[pe_direction[0]]]
        axcode_index = improper_ax_idcs[pe_direction[0]]
        inv = pe_direction[1:] == "-"

        if pe_direction[0] == "x":
            if "L" in axcode:
                inv = not inv
        elif pe_direction[0] == "y":
            if "P" in axcode:
                inv = not inv
        elif pe_direction[0] == "z":
            if "I" in axcode:
                inv = not inv
        else:
            ValueError("pe_direction does not contain letter i, j, k, x, y, or z")

        if inv:
            polarity = "-"
        else:
            polarity = ""

        proper_pe_direction = [key for key, value in proper_ax_idcs.items()
                               if value == axcode_index][0] + polarity

    return proper_pe_direction


def determine_direction(pe_direction, ornt):
    """
    Takes [corrected] pe_direction (pe_direction) and image orientation (ornt)
    to determine "_dir-" entity label, which is required for specific
    acquisitions.

    Based on https://github.com/nipreps/fmriprep/issues/2341 and original code
    comes from Chris Markiewicz and Mathias Goncalves.

    Parameters
    ----------
    pe_direction : string
        Value from PhaseEncodingDirection in acquisition json file generated
        by dcm2niix
    ornt: string
        Value of "".join(nib.aff2axcodes(nii_img.affine)), where "nii_img" is
        is the acquisition NIFTI file generated by dcm2niix

    Returns
    -------
    direction: string
        direction for BIDS "_dir-" entity label
    """
    axes = (("R", "L"), ("A", "P"), ("S", "I"))
    ax_idcs = {"i": 0, "j": 1, "k": 2}
    axcode = ornt[ax_idcs[pe_direction[0]]]
    inv = pe_direction[1:] == "-"

    if pe_direction[0] == "i":
        if "L" in axcode:
            inv = not inv
    elif pe_direction[0] == "j":
        if "P" in axcode:
            inv = not inv
    elif pe_direction[0] == "k":
        if "I" in axcode:
            inv = not inv

    for ax in axes:
        for flip in (ax, ax[::-1]):
            if flip[not inv].startswith(axcode):
                direction = "".join(flip)

    return direction


def modify_uploaded_dataset_list(uploaded_json_list):
    """
    Filters the list of json files generated by preprocess.sh to ensure that
    the json files are derived from dcm2niix, and that they contain
    corresponding nifti (and bval/bvec) files. Additionally, Phillips parrec
    files are removed, as they cannot be handled by ezBIDS. If these conditions
    are satisfied, all files are added to a modified dir_list.

    Parameters
    ----------
    uploaded_json_list : list
        list of json files generated from preprocess.sh

    Returns
    -------
    uploaded_files_list: list
        all files (i.e json, nifti, bval/bvec) from uploaded dataset
    """
    uploaded_files_list = []

    # Remove Philips proprietary files in uploaded_json_list if they exist
    uploaded_json_list = [json for json in uploaded_json_list
                          if "parrec" not in json.lower()]

    # Sort uploaded_json_list
    uploaded_json_list.sort()

    # Parse json files
    for json_file in uploaded_json_list:
        json_data = open(json_file)
        json_data = json.load(json_data, strict=False)

        # Only want json files with corresonding nifti (and bval/bvec) and if
        # the files come from dcm2niix
        if "ConversionSoftware" in json_data and json_data["ConversionSoftware"] == "dcm2niix":
            if len([os.path.dirname(json_file) + "/" + x for x in os.listdir(os.path.dirname(json_file)) if os.path.basename(json_file)[:-4] in x]) > 1:
                uploaded_files_list.append([os.path.dirname(json_file) + "/" + x for x in os.listdir(os.path.dirname(json_file)) if os.path.basename(json_file[:-4]) in x])

    # Flatten uploaded_dataset_list
    uploaded_files_list = [file for sublist in uploaded_files_list
                           for file in sublist]

    return uploaded_files_list


def create_screenshots(nifti_path, nibabel_nifti_obj, num_volumes):
    """
    Generates PNG screenshot files of each acquisition

    Parameters
    ----------
    nifti_path: str
        location of nifti file

    nibabel_nifti_obj : nibabel.nifti1.Nifti1Image
        result of nib.load(nifti_file_path)

    num_volumes: int
        number of volumes in the nifti file

    Returns
    -------
    None
    """
    image = nibabel_nifti_obj
    object_img_array = image.dataobj

    if num_volumes > 1:
        object_img_array = image.dataobj[..., 1]
    else:
        object_img_array = image.dataobj[:]

    slice_x = object_img_array[floor(object_img_array.shape[0]/2), :, :]
    slice_y = object_img_array[:, floor(object_img_array.shape[1]/2), :]
    slice_z = object_img_array[:, :, floor(object_img_array.shape[2]/2)]

    fig, axes = plt.subplots(1, 3, figsize=(9, 3))
    for index, slices in enumerate([slice_x, slice_y, slice_z]):
        axes[index].imshow(slices.T, cmap="gray", origin="lower", aspect="auto")
        axes[index].axis("off")
    plt.tight_layout(pad=0, w_pad=0, h_pad=0)

    # draw the renderer
    fig.canvas.draw()

    # Get the RGBA buffer from the figure
    w,h = fig.canvas.get_width_height()
    buf = np.fromstring(fig.canvas.tostring_argb(), dtype=np.uint8)
    buf.shape = (w,h,4)

    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode
    buf = np.roll(buf,3,axis=2)

    w,h,d = buf.shape
    png = Image.frombytes("RGBA", (w,h), buf.tostring())
    png.save("{}.png".format(nifti_path[:-7]))

def generate_dataset_list(uploaded_files_list):
    """
    Takes list of nifti, json, (and bval/bvec) files generated from dcm2niix
    to create a list of info directories for each uploaded acquisition, where
    each directory contains metadata and other dicom header information to
    help ezBIDS determine the identify of acquisitions, and to determine other
    BIDS-related information (e.g. entitiy labels).

    Parameters
    ----------
    uploaded_files_list : list
        List of nifti, json, and bval/bvec files generated from dcm2niix. The
        list of files is generated from preprocess.sh

    Returns
    -------
    dataset_list: list
        List of dictionaries containing pertinent and unique information about
        the data, primarily coming from the metadata in the json files
    """
    # Create list for appending dictionaries to
    dataset_list = []

    # Get separate nifti and json (i.e. sidecar) lists
    json_list = [x for x in uploaded_files_list if ".json" in x]
    nifti_list = [x for x in uploaded_files_list
                  if ".nii.gz" in x
                  or ".bval" in x
                  or ".bvec" in x]

    print("Determining unique acquisitions in dataset")
    print("------------------------------------------")
    for index, json_file in enumerate(json_list):
        json_data = open(json_file)
        json_data = json.load(json_data, strict=False)

        corresponding_nifti = [x for x in nifti_list if json_file[:-4] in x
                               if "nii.gz" in x][0]

        #Phase encoding direction info
        if "PhaseEncodingDirection" in json_data:
            pe_direction = json_data["PhaseEncodingDirection"]
        else:
            pe_direction = None

        try:
            ornt = nib.aff2axcodes(nib.load(corresponding_nifti).affine)
            ornt = "".join(ornt)
        except:
            ornt = None

        if pe_direction is not None and ornt is not None:
            proper_pe_direction = correct_pe(pe_direction, ornt)
            ped = determine_direction(proper_pe_direction, ornt)
        else:
            ped = ""

        # Nifti (and bval/bvec) file(s) associated with specific json file
        nifti_paths_for_json = [x for x in nifti_list if json_file[:-4] in x]
        nifti_paths_for_json = [x for x in nifti_paths_for_json if ".json" not in x]

        # Find nifti file size
        filesize = os.stat(nifti_paths_for_json[0]).st_size

        # Find StudyID from json
        if "StudyID" in json_data:
            study_id = json_data["StudyID"]
        else:
            study_id = ""

        # Find subject_id from json, since some files contain
        # neither PatientName nor PatientID
        if "PatientName" in json_data:
            patient_name = json_data["PatientName"]
        else:
            patient_name = os.path.dirname(json_file)

        if "PatientID" in json_data:
            patient_id = json_data["PatientID"]
        else:
            patient_id = os.path.basename(json_file)

        # Find PatientBirthDate
        if "PatientBirthDate" in json_data:
            patient_birth_date = json_data["PatientBirthDate"].replace("-", "")
        else:
            patient_birth_date = "00000000"

        # Find PatientSex
        if "PatientSex" in json_data:
            patient_sex = json_data["PatientSex"]
            if patient_sex not in ["M", "F"]:
                patient_sex = "N/A"
        else:
            patient_sex = "N/A"

        # Select subject_id to display to ezBIDS users.
        # Precedence order: PatientName > PatientID > PatientBirthDate
        if patient_name:
            subject = patient_name
        elif patient_id:
            subject = patient_id
        elif patient_birth_date:
            subject = patient_birth_date
        else:
            subject = "NA"

        if subject != "NA":
            if "sub_" in subject or "sub-" in subject:
                split = subject.split("sub")[-1][1:]
                subject = split[0:re.search(r"[^A-Za-z0-9]+", split).start()]
            else:
                subject = re.sub("[^A-Za-z0-9]+", "", subject)

        # Find Acquisition Date & Time
        if "AcquisitionDateTime" in json_data:
            acquisition_date = json_data["AcquisitionDateTime"].split("T")[0]
            acquisition_time = json_data["AcquisitionDateTime"].split("T")[-1]
        else:
            acquisition_date = "0000-00-00"
            acquisition_time = None

        # Find RepetitionTime
        if "RepetitionTime" in json_data:
            repetition_time = json_data["RepetitionTime"]
        else:
            repetition_time = "N/A"

        # Find EchoNumber
        if "EchoNumber" in json_data:
            echo_number = json_data["EchoNumber"]
        else:
            echo_number = None

        # Find EchoTime
        if "EchoTime" in json_data:
            echo_time = json_data["EchoTime"]*1000
        else:
            echo_time = 0

        # get the nibabel nifti image info
        image = nib.load(json_file[:-4] + "nii.gz")

        # Find how many volumes are in jsons's corresponding nifti file
        try:
            volume_count = image.shape[3]
        except:
            volume_count = 1

        # Relative paths of json and nifti files (per SeriesNumber)
        paths = sorted(nifti_paths_for_json + [json_file])

        # Organize all from individual SeriesNumber in dictionary
        acquisition_info_directory = {
            "StudyID": study_id,
            "PatientName": patient_name,
            "PatientID": patient_id,
            "PatientBirthDate": patient_birth_date,
            "PatientSex": patient_sex,
            "PatientAge": "N/A",
            "subject": subject,
            "session": "",
            "SeriesNumber": json_data["SeriesNumber"],
            "AcquisitionDate": acquisition_date,
            "AcquisitionTime": acquisition_time,
            "SeriesDescription": json_data["SeriesDescription"],
            "ProtocolName": json_data["ProtocolName"],
            "ImageType": json_data["ImageType"],
            "RepetitionTime": repetition_time,
            "EchoNumber": echo_number,
            "EchoTime": echo_time,
            "DataType": "",
            "ModalityLabel": "",
            "series_idx": 0,
            "direction": ped,
            "TaskName": "",
            "exclude": False,
            "filesize": filesize,
            "NumVolumes": volume_count,
            "forType": "",
            "error": None,
            "section_ID": 1,
            "message": [],
            "br_type": "",
            "nifti_path": [x for x in nifti_paths_for_json if ".nii.gz" in x][0],
            'nibabel_image': image,
            "json_path": json_file,
            "paths": paths,
            "pngPath": "",
            "headers": "",
            "sidecar":json_data
        }
        dataset_list.append(acquisition_info_directory)

    # Sort dataset_list of dictionaries by subject, AcquisitionDate,
    # SeriesNumber, and json_path.
    dataset_list = sorted(dataset_list, key=itemgetter("subject",
                                                       "AcquisitionDate",
                                                       "SeriesNumber",
                                                       "json_path"))

    return dataset_list

def determine_subj_ses_IDs(dataset_list):
    """
    Determine subject ID(s), and session ID(s) (if applicable) of uploaded
    dataset.

    Parameters
    ----------
    dataset_list: list
        List of dictionaries containing pertinent and unique information about
        the data, primarily coming from the metadata in the json files

    Returns
    -------
    dataset_list: list
        List of dictionaries containing pertinent and unique information about
        the data, primarily coming from the metadata in the json files

    subject_ids_info: list
        List of dictionaries containing subject identification info, such as
        PatientID, PatientName, PatientBirthDate, and corresponding session
        information

    """
    # Curate subject_id information
    subject_ids_info = list({"subject":x["subject"],
                             "PatientID":x["PatientID"],
                             "PatientName":x["PatientName"],
                             "PatientBirthDate":x["PatientBirthDate"],
                             "AcquisitionDate":x["AcquisitionDate"],
                             "phenotype":{
                                 "sex":x["PatientSex"],
                                 "age":x["PatientAge"]},
                             "exclude": False,
                             "sessions": [],
                             "validationErrors": []} for x in dataset_list)

    # Create modified list of dictionary with unique subject and Acquisition
    # values
    subject_ids_info_mod = list({(v["subject"], v["AcquisitionDate"]):v
                                 for v in subject_ids_info}.values())

    # Create list of dictionaries with unique subject values. This list of
    # dictionaries will be used by ezBIDS UI.
    subject_ids_info = list({v["subject"]:v for v in subject_ids_info}.values())

    # Unique subject IDs in dataset
    subj_ids = [x["subject"] for x in subject_ids_info]

    # Determine and apply session ID value if possible
    for subj_id in subj_ids:
        subject_indices = [index for index, dictionary
                           in enumerate(subject_ids_info_mod)
                           if dictionary["subject"] == subj_id]

        if len(subject_indices):
            sessions_info = []

        for index, subj_index in enumerate(subject_indices):


            if len(subject_indices) > 1:
                session_id = str(index+1)
            else:
                session_id = ""

            sessions_info.append({"AcquisitionDate": subject_ids_info_mod[subj_index]["AcquisitionDate"],
                                  "session": session_id,
                                  "exclude": False})
            # Apply the session information to the correct subject dictionaries
            subj_dictionary = [x for x in subject_ids_info
                               if x["subject"] == subj_id][0]

            subj_dictionary["sessions"] = sessions_info

    # Remove AcquisitionDate key from dictionaries (info contained in sessions)
    for dic in subject_ids_info:
        del dic["AcquisitionDate"]

    # Add the session ID to the dataset_list dictionaries
    for acquisition_dic in dataset_list:
        for subject_dic in subject_ids_info:
            if subject_dic["subject"] == acquisition_dic["subject"] and len(subject_dic["sessions"]) > 1:
                for session in subject_dic["sessions"]:
                    if acquisition_dic["AcquisitionDate"] == session["AcquisitionDate"]:
                        acquisition_dic["session"] = session["session"]

    return dataset_list, subject_ids_info


def determine_unique_series(dataset_list):
    """
    From the dataset_list, lump the individual acquisitions into unique series.
    Unique data is determined from 4 dicom header values: SeriesDescription
    EchoTime, ImageType, and RepetitionTime. If EchoTime values differ
    slightly (+/- 1) and other values are the same, a unique series ID is not
    given, since EchoTime is the only dicom headeh with continuous values.

    Parameters
    ----------
    dataset_list: list
        List of dictionaries containing pertinent and unique information about
        the data, primarily coming from the metadata in the json files.

    Returns
    -------
    dataset_list_unique_series: list
        A modified version of dataset_list, where the list contains only the
        dictionaries of acquisitions with a unique series ID.
    """
    dataset_list_unique_series = []
    series_checker = []
    series_idx = 0

    for index, acquisition_dic in enumerate(dataset_list):
        """
        If retro-reconstruction (RR) acquistions are found
        ("_RR" in SeriesDescription), they should be part of same unique
        series as non retro-reconstruction ones. These are generally rare
        cases, but should be accounted for.
        """
        if "_RR" in acquisition_dic["SeriesDescription"]:
            modified_sd = acquisition_dic["SeriesDescription"].replace("_RR", "")
            heuristic_items = [acquisition_dic["EchoTime"],
                               modified_sd,
                               acquisition_dic["ImageType"],
                               acquisition_dic["RepetitionTime"],
                               1]
        else:
            heuristic_items = [acquisition_dic["EchoTime"],
                               acquisition_dic["SeriesDescription"],
                               acquisition_dic["ImageType"],
                               acquisition_dic["RepetitionTime"],
                               1]

        if index == 0:
            acquisition_dic["series_idx"] = 0
            dataset_list_unique_series.append(acquisition_dic)

        # unique acquisition, make unique series ID
        elif heuristic_items not in [x[:-1] for x in series_checker]:
            # But first, check if EchoTimes are essentially the same (i.e. +- 0.5)
            if heuristic_items[1:] in [x[1:-1] for x in series_checker]:
                echo_time = heuristic_items[0]
                common_series_index = [x[1:-1] for x in series_checker].index(heuristic_items[1:])

                if series_checker[common_series_index][0] - 0.5 <= echo_time <= series_checker[common_series_index][0] + 0.5:
                    common_series_idx = series_checker[common_series_index][-1]
                    acquisition_dic["series_idx"] = common_series_idx
                else:
                    series_idx += 1
                    acquisition_dic["series_idx"] = series_idx
                    dataset_list_unique_series.append(acquisition_dic)

            else:
                series_idx += 1
                acquisition_dic["series_idx"] = series_idx
                dataset_list_unique_series.append(acquisition_dic)

        else:
            common_series_index = [x[:-1] for x in series_checker].index(heuristic_items)
            common_series_idx = series_checker[common_series_index][-1]
            acquisition_dic["series_idx"] = common_series_idx

        series_checker.append(heuristic_items + [acquisition_dic["series_idx"]])

    return dataset_list, dataset_list_unique_series


def identify_series_info(dataset_list_unique_series):
    """
    Takes list of dictionaries with key and unique information, and uses it to
        determine the DataType and Modality labels of the unique acquisitions.
        Other information (e.g. run, acq, ce) will be determined if the data
        follows the ReproIn naming convention for SeriesDescriptions.

    Parameters
    ----------
    dataset_list_unique_series: list
        List of dictionaries containing pertinent about the unique
        acquisitions. This information is displayed to the user through the
        UI, which grabs this information

    Returns
    -------
    unique_series_list : list
        List of dictionaries containing key information about the uploaded data.
        Only acquisitions with a unique series ID are included.
    """
    # Determine DataType, ModalityLabel, and BIDS entity labels for the unique
    # series IDs.
    unique_series_list = []
    for index, unique_dic in enumerate(dataset_list_unique_series):

        series_entities = {}
        sd = unique_dic["SeriesDescription"]
        image_type = unique_dic["ImageType"]
        echo_time = unique_dic["EchoTime"]

        if "SequenceName" in unique_dic["sidecar"]:
            sequence_name = unique_dic["sidecar"]["SequenceName"]
        elif "ScanningSequence" in unique_dic["sidecar"]:
            sequence_name = unique_dic["sidecar"]["ScanningSequence"]
        else:
            sequence_name = "N/A"

        # If ReproIn convention was used for dataset, populate as much
        # information as possible.
        if "sub-" in sd:
            series_entities["subject"] = sd.split("sub-")[-1].split("_")[0]
        else:
            series_entities["subject"] = None

        if "_ses-" in sd:
            series_entities["session"] = sd.split("_ses-")[-1].split("_")[0]
        else:
            series_entities["session"] = None

        if "_run-" in sd:
            series_entities["run"] = sd.split("_run-")[-1].split("_")[0]
            if series_entities["run"][0] == "0":
                series_entities["run"] = series_entities["run"][1:]
        else:
            series_entities["run"] = ""

        if "_task-" in sd:
            series_entities["task"] = sd.split("_task-")[-1].split("_")[0]
        else:
            pass

        if "_dir-" in sd:
            series_entities["direction"] = sd.split("_dir-")[-1].split("_")[0]
        else:
            series_entities["direction"] = ""

        if "_acq-" in sd:
            series_entities["acquisition"] = sd.split("_acq-")[-1].split("_")[0]
        else:
            series_entities["acquisition"] = ""

        if "_ce-" in sd:
            series_entities["ceagent"] = sd.split("_ce-")[-1].split("_")[0]
        else:
            series_entities["ceagent"] = ""

        if "_echo-" in sd:
            series_entities["echo"] = sd.split("_echo-")[-1].split("_")[0]
            if series_entities["echo"][0] == "0":
                series_entities["echo"] = series_entities["echo"][1:]
        else:
            series_entities["echo"] = ""

        if "_fa-" in sd:
            series_entities["fa"] = sd.split("_fa-")[-1].split("_")[0]
        else:
            series_entities["fa"] = ""

        if "_inv-" in sd:
            series_entities["inversion"] = sd.split("_inv-")[-1].split("_")[0]
            if series_entities["inversion"][0] == "0":
                series_entities["inversion"] = series_entities["inversion"][1:]
        else:
            series_entities["inversion"] = ""

        if "_part-" in sd:
            series_entities["part"] = sd.split("_part-")[-1].split("_")[0]
        else:
            series_entities["part"] = ""


        # Make easier to find key characters/phrases in sd by removing
        # non-alphanumeric characters and make everything lowercase.
        sd = re.sub("[^A-Za-z0-9]+", "", sd).lower()

        # Keys for helping determine acquisition types, based
        # on SeriesDescription.
        localizer_keys = ["localizer", "scout"]
        asl_keys = ["asl"]
        angio_keys = ["angio"]
        se_mag_phase_fmap_keys = ["fmap", "fieldmap", "spinecho", "sefmri", "semri"]
        flair_keys = ["flair", "t2spacedafl"]
        dwi_derived_keys = ["trace", "fa", "adc"]
        dwi_keys = ["dti", "dwi", "dmri"]
        func_keys = ["bold", "func", "fmri", "epi", "mri", "task", "rest"]
        func_rest_keys = ["rest", "rsfmri", "fcmri"]
        t1w_keys = ["t1w", "tfl3d", "mprage", "spgr", "tflmgh"]
        t2w_keys = ["t2w", "t2"]
        additional_anat_keys = ["t2starw", "inplanet1", "inplanet2", "pdt2", "pdw"]
        anat_parametric_keys = ["pdt2map", "t2starmap", "r2starmap",
                                "mwfmap", "mtvmap", "chimap", "tb1map",
                                "pdmap", "mtrmap", "mtsat", "t1rho",
                                "rb1map", "s0map", "m0map", "t1map", "r1map",
                                "t2map", "r2map"]

        # # #  Determine DataTypes and ModalityLabels # # # # # # #

        # Localizers
        if any(x in sd for x in localizer_keys) or sd == "tfl":
            unique_dic["error"] = "Acquisition appears to be a localizer"
            try:
                unique_dic["message"] = " ".join("Acquisition is believed to be a localizer \
                    because '{}' is in the SeriesDescription. Please modify if \
                    incorrect.".format([x for x in localizer_keys if re.findall(x, sd)][0]).split())
            except:
                 unique_dic["message"] = " ".join("Acquisition is believed to \
                    be a localizer because the SeriesDescription is 'tfl'. Please modify if incorrect.".split())
            unique_dic["br_type"] = "exclude"

        # Arterial Spin Labeling (ASL)
        elif any(x in sd for x in asl_keys):
            unique_dic["br_type"] = "exclude"
            unique_dic["DataType"] = "asl"
            unique_dic["ModalityLabel"] = "asl"
            unique_dic["error"] = " ".join("Acqusition appears to be ASL, which is \
                currently not supported by ezBIDS at this time, but will be \
                in the future".split())
            unique_dic["message"] = " ".join("Acquisition is believed to be asl/asl \
                because '{}' is in the SeriesDescription. Please modify if \
                incorrect. Currently, ezBIDS does not support ASL conversion \
                to BIDS".format([x for x in asl_keys if re.findall(x, sd)][0]).split())

        # Angiography
        elif any(x in sd for x in angio_keys):
            unique_dic["br_type"] = "exclude"
            unique_dic["DataType"] = "anat"
            unique_dic["ModalityLabel"] = "angio"
            unique_dic["error"] = " ".join("Acqusition appears to be an Angiography \
                acquisition, which is currently not supported by ezBIDS at \
                this time, but will be in the future".split())
            unique_dic["message"] = " ".join("Acquisition is believed to be anat/angio \
                because '{}' is in the SeriesDescription. Please modify if \
                incorrect. Currently, ezBIDS does not support Angiography \
                conversion to BIDS".format([x for x in angio_keys if re.findall(x, sd)][0]).split())

        # Magnitude/Phase[diff] and Spin Echo (SE) field maps
        elif any(x in sd for x in se_mag_phase_fmap_keys):
            unique_dic["DataType"] = "fmap"
            unique_dic["forType"] = "func/bold"
            if "REAL" in unique_dic["ImageType"]:
                series_entities["part"] = "real"
            if "IMAGINARY" in unique_dic["ImageType"]:
                series_entities["part"] = "imag"

            # Magnitude/Phase[diff] field maps
            if "EchoNumber" in unique_dic["sidecar"]:
                if any(x in unique_dic["json_path"] for x in ["_real.", "_imaginary."]):
                    unique_dic["error"] = " ".join("Acquisition appears to be a real or \
                        imaginary field map that needs to be manually \
                        adjusted to magnitude and phase (ezBIDS currently \
                        does not have this functionality). This acqusition \
                        will not be converted".split())
                    unique_dic["message"] = unique_dic["error"]
                    unique_dic["br_type"] = "exclude"
                elif unique_dic["EchoNumber"] == 1 and "_e1_ph" not in unique_dic["json_path"]:
                    unique_dic["ModalityLabel"] = "magnitude1"
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        fmap/magnitude1 because '{}' is in SeriesDescription, \
                        EchoNumber == 1 in metadata, and the phrase '_e1_ph' \
                        is not in the filename. Please modify if \
                        incorrect".format([x for x in se_mag_phase_fmap_keys if re.findall(x, sd)][0]).split())
                elif unique_dic["EchoNumber"] == 1 and "_e1_ph" in unique_dic["json_path"]:
                    unique_dic["ModalityLabel"] = "phase1"
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        fmap/phase1 because '{}' is in SeriesDescription, \
                        EchoNumber == 1 in metadata, and the phrase '_e1_ph' \
                        is in the filename. Please modify if \
                        incorrect".format([x for x in se_mag_phase_fmap_keys if re.findall(x, sd)][0]).split())
                elif unique_dic["EchoNumber"] == 2 and "_e2_ph" not in unique_dic["json_path"]:
                    unique_dic["ModalityLabel"] = "magnitude2"
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        fmap/magnitude2 because '{}' is in SeriesDescription, \
                        EchoNumber == 2 in metadata, and the phrase '_e2_ph' \
                        is not in the filename. Please modify if \
                        incorrect".format([x for x in se_mag_phase_fmap_keys if re.findall(x, sd)][0]).split())
                elif unique_dic["EchoNumber"] == 2 and "_e2_ph" in unique_dic["json_path"] and "_e1_ph" in dataset_list_unique_series[index-2]["json_path"]:
                    unique_dic["ModalityLabel"] = "phase2"
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        fmap/phase2 because '{}' is in SeriesDescription, \
                        EchoNumber == 2 in metadata, and the phrase '_e2_ph' \
                        is in the filename and '_e1_ph' the one two before. \
                        Please modify if incorrect".format([x for x in se_mag_phase_fmap_keys if re.findall(x, sd)][0]).split())
                elif unique_dic["EchoNumber"] == 2 and "_e2_ph" in unique_dic["json_path"] and "_e1_ph" not in dataset_list_unique_series[index-2]["json_path"]:
                    unique_dic["ModalityLabel"] = "phasediff"
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        fmap/phasediff because 'fmap' or 'fieldmap' is in \
                        SeriesDescription, EchoNumber == 2 in metadata, and \
                        the subjectstring '_e2_ph' is in the filename but \
                        '_e1_ph' not found in the acquisition two before. \
                        Please modify if incorrect".split())
                else:
                    unique_dic["error"] = " ".join("Acquisition appears to be some form \
                        of fieldmap with an EchoNumber, however, unable to \
                        determine if it is a magnitude, phase, or phasediff. \
                        Please modify if acquisition is desired for BIDS \
                        conversion, otherwise the acqusition will not be \
                        converted".split())
                    unique_dic["message"] = unique_dic["error"]
                    unique_dic["br_type"] = "exclude"

            # Spin echo field maps
            else:
                unique_dic["ModalityLabel"] = "epi"
                unique_dic["message"] = " ".join("Acquisition is believed to be fmap/epi \
                    because '{}' is in SeriesDescription, and does not \
                    contain metadata info associated with magnitude/phasediff \
                    acquisitions. Please modify if incorrect".format([x for x in se_mag_phase_fmap_keys if re.findall(x, sd)][0]).split())
                series_entities["direction"] = unique_dic["direction"]

        # DWI
        elif "DIFFUSION" in unique_dic["ImageType"] and "b0" in sd:
            unique_dic["DataType"] = "fmap"
            unique_dic["ModalityLabel"] = "epi"
            unique_dic["forType"] = "dwi/dwi"
            series_entities["direction"] = unique_dic["direction"]
            unique_dic["message"] = " ".join("Acquisition appears to be a fmap/epi meant \
                for dwi/dwi, as 'DIFFUSION' is in ImageType, and 'b0' is in \
                the SeriesDescription. Please modify if incorrect".split())

        elif not any(".bvec" in x for x in unique_dic["paths"]) and "DIFFUSION" in unique_dic["ImageType"]:
            unique_dic["error"] = " ".join("Acquisitions has 'DIFFUSION' label in the \
                ImageType; however, there are no corresponding bval/bvec \
                files. This may or may not be dwi/dwi. Please modify if \
                incorrect.".split())
            unique_dic["message"] = unique_dic["error"]
            unique_dic["br_type"] = "exclude"

        elif any(".bvec" in x for x in unique_dic["paths"]):
            if "DIFFUSION" not in unique_dic["ImageType"]:
                if unique_dic["NumVolumes"] < 2:
                    if any(x in sd for x in flair_keys):
                        unique_dic["DataType"] = "anat"
                        unique_dic["ModalityLabel"] = "FLAIR"
                        unique_dic["message"] = " ".join("Acquisition is believed to be \
                            anat/FLAIR because '{}' is in the \
                            SeriesDescription. Please modify if incorrect".format([x for x in flair_keys if re.findall(x, sd)][0]).split())
                    elif "t2w" in sd:
                        unique_dic["DataType"] = "anat"
                        unique_dic["ModalityLabel"] = "T2w"
                        unique_dic["message"] = " ".join("Acquisition is believed to be \
                            anat/T2w because 't2w' is in the \
                            SeriesDescription. Please modify if incorrect".split())
                    else:
                        unique_dic["error"] = " ".join("Acquisition has bval and bvec \
                            files but does not appear to be dwi/dwi because \
                            'DIFUSSION' is not in ImageType and contains less \
                            than 2 volumes. Please modify if incorrect, \
                            otherwise will not convert to BIDS".split())
                        unique_dic["message"] = unique_dic["error"]
                        unique_dic["br_type"] = "exclude"
                else:
                    unique_dic["DataType"] = "dwi"
                    unique_dic["ModalityLabel"] = "dwi"
                    unique_dic["message"] = " ".join("Acquisition appears to be dwi/dwi \
                        because although 'DIFUSSION' is not in ImageType, the \
                        acquisition has bval and bvec files and has {} \
                        volumes. Please modify if incorrect".format(unique_dic["NumVolumes"]).split())
                    series_entities["direction"] = unique_dic["direction"]
            else:
                # Low b-values will default to fmap/epi, intended to be used
                # on dwi/dwi data.
                bval = np.loadtxt([x for x in unique_dic["paths"] if "bval" in x][0])
                if np.max(bval) <= 50:
                    unique_dic["DataType"] = "fmap"
                    unique_dic["ModalityLabel"] = "epi"
                    series_entities["direction"] = unique_dic["direction"]
                    unique_dic["forType"] = "dwi/dwi"
                    unique_dic["message"] = " ".join("Acquisition appears to be \
                        fmap/epi meant for dwi/dwi, as there are bval & bvec \
                        files, but with low b-values. Please modify if \
                        incorrect".split())

                # elif any(x in sd for x in dwi_derived_keys) and not any(x in sd for x in dwi_keys):
                elif any(x in sd for x in dwi_derived_keys):
                    unique_dic["error"] = " ".join("Acquisition appears to be a TRACE, \
                        FA, or ADC, which are unsupported by ezBIDS and will \
                        therefore not be converted".split())
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        TRACE, FA, or ADC because there are bval & bvec files \
                        with the same SeriesNumber, and '{}' are in the \
                        SeriesDescription. Please modify if \
                        incorrect".format([x for x in dwi_derived_keys if re.findall(x, sd)][0]).split())
                    unique_dic["br_type"] = "exclude"
                else:
                    unique_dic["DataType"] = "dwi"
                    unique_dic["ModalityLabel"] = "dwi"
                    series_entities["direction"] = unique_dic["direction"]
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        dwi/dwi because there are bval & bvec files with the \
                        same SeriesNumber, 'DIFFUSION' is in the ImageType, \
                        and it does not appear to be derived dwi data. Please \
                        modify if incorrect".split())

        # DWI derivatives or other non-BIDS diffusion offshoots
        elif any(x in sd for x in dwi_derived_keys) and any(x in sd for x in dwi_keys):
            unique_dic["error"] = " ".join("Acquisition appears to be a TRACE, FA, or \
                ADC, which are unsupported by ezBIDS and will therefore not \
                be converted".split())
            unique_dic["message"] = " ".join("Acquisition is believed to be dwi-derived \
                (TRACE, FA, ADC), which are not supported by BIDS and will not \
                be converted. Please modify if incorrect".split())
            unique_dic["br_type"] = "exclude"

        # Functional bold and phase
        elif any(x in sd for x in func_keys) and "sbref" not in sd:
            if unique_dic["NumVolumes"] < 50:
                unique_dic["br_type"] = "exclude"
                unique_dic["message"] = " ".join("Acquisition appears to be functional \
                    but contain less than 50 volumes, suggesting a \
                    failure/restart, or possibly a test acquisition. Please \
                    modify if incorrect".split())
            else:
                unique_dic["DataType"] = "func"
                if any(x in sd for x in func_rest_keys):
                    series_entities["task"] = "rest"
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        func/bold because '{}' is in the SeriesDescription \
                        (but not 'sbref'). Please modify if incorrect".format([x for x in func_keys if re.findall(x, sd)][0]).split())
                if "MOSAIC" and "PHASE" in unique_dic["ImageType"]:
                    unique_dic["ModalityLabel"] = "bold"
                    series_entities["part"] = "phase"
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        func/bold (part-phase) because '{}' is in the \
                        SeriesDescription (but not 'sbref'), and 'MOSAIC' \
                        and 'PHASE' are in the ImageType field of the metadata.\
                        Please modify if incorrect".format([x for x in func_keys if re.findall(x, sd)][0]).split())
                else:
                    unique_dic["ModalityLabel"] = "bold"
                    if unique_dic["EchoNumber"]:
                        series_entities["echo"] = unique_dic["EchoNumber"]
                        unique_dic["message"] = " ".join("Acquisition is believed to be \
                            multiecho func/bold because '{}' is in the \
                            SeriesDescription (but not 'sbref'), and \
                            EchoTime == {}. Please modify if incorrect".format([x for x in func_keys if re.findall(x, sd)][0], unique_dic["EchoNumber"]).split())
                if unique_dic["EchoNumber"]:
                    series_entities["echo"] = unique_dic["EchoNumber"]
                    unique_dic["message"] = " ".join("Acquisition is believed to be \
                        multiecho func/bold because '{}' is in the \
                        SeriesDescription (but not 'sbref'), and \
                        EchoTime == {}. Please modify if incorrect".format([x for x in func_keys if re.findall(x, sd)][0], unique_dic["EchoNumber"]).split())

        # Functional single band reference (sbref)
        elif "sbref" in sd:
            unique_dic["ModalityLabel"] = "sbref"

            if "DIFFUSION" in unique_dic["ImageType"]:
                unique_dic["DataType"] = "dwi"
            else:
                unique_dic["DataType"] = "func"

                if any(x in sd for x in func_rest_keys):
                    series_entities["task"] = "rest"
                if unique_dic["EchoNumber"]:
                    series_entities["echo"] = unique_dic["EchoNumber"]
                unique_dic["message"] = " ".join("Acquisition is believed to be \
                    func/sbref because 'sbref' is in the SeriesDescription".split())

        # MP2RAGE/UNIT1
        elif "mp2rage" in sd:
            unique_dic["DataType"] = "anat"

            if "InversionTime" in unique_dic["sidecar"]:
                inversion_time = unique_dic["sidecar"]["InversionTime"]
            else:
                inversion_time = None

            if inversion_time:
                unique_dic["ModalityLabel"] = "MP2RAGE"
                unique_dic["message"] = " ".join("Acquisition is believed to be \
                    anat/mpr2rage, but 'mp2rage' in in the SeriesDescription, \
                    and there is an InversionTime in the metadata. Please \
                    modify if incorrect.".split())

                if inversion_time < 1.0:
                    series_entities["inversion"] = 1
                else:
                    series_entities["inversion"] = 2

                # Look for echo number
                if "EchoNumber" in unique_dic["sidecar"]:
                    series_entities["echo"] = unique_dic["sidecar"]["EchoNumber"]

                # Determine part value (mag/phase)
                if "_e2.json" in unique_dic["json_path"]:
                    series_entities["part"] = "phase"
                else:
                    series_entities["part"] = "mag"

            else:
                unique_dic["ModalityLabel"] = "UNIT1"
                unique_dic["message"] = " ".join("Acquisition is believed to be \
                    anat/UNIT1, but 'mp2rage' in in the SeriesDescription, \
                    but there is no InversionTime in the metadata. Please \
                    modify if incorrect.".split())


        # T1w
        elif any(x in sd for x in t1w_keys):
            unique_dic["DataType"] = "anat"
            unique_dic["ModalityLabel"] = "T1w"
            if "multiecho" in sd or "echo" in sd:
                if "MEAN" not in image_type:
                    series_entities["echo"] = unique_dic["EchoNumber"]
            unique_dic["message"] = " ".join("Acquisition is believed to be anat/T1w \
                because '{}' is in the SeriesDescription. Please modify if \
                incorrect".format([x for x in t1w_keys if re.findall(x, sd)][0]).split())

        # FLAIR
        elif any(x in sd for x in flair_keys):
            unique_dic["DataType"] = "anat"
            unique_dic["ModalityLabel"] = "FLAIR"
            unique_dic["message"] = " ".join("Acquisition is believed to be anat/FLAIR \
                because '{}' is in the SeriesDescription. Please modify if \
                incorrect".format([x for x in flair_keys if re.findall(x, sd)][0]).split())

        # T2w
        #T2w acquisitions typically have EchoTime > 100ms
        elif any(x in sd for x in t2w_keys) and unique_dic["EchoTime"] > 100:
            unique_dic["DataType"] = "anat"
            unique_dic["ModalityLabel"] = "T2w"
            unique_dic["message"] = " ".join("Acquisition is believed to be anat/T2w \
                because '{}' is in the SeriesDescription and EchoTime > 100ms. \
                Please modify if incorrect".format([x for x in t2w_keys if re.findall(x, sd)][0]).split())


        # Anatomical non-parametric
        elif any(x in sd for x in additional_anat_keys):
            unique_dic["DataType"] = "anat"
            if "t2starw" in sd:
                unique_dic["ModalityLabel"] = "T2starw"
                unique_dic["message"] = " ".join("Acquisitions is believed to be \
                    anat/T2starw because 'T2starw' is in the SeriesDescription. \
                    Please modify if incorrect".split())
            elif "inplanet1" in sd:
                unique_dic["ModalityLabel"] = "inplaneT1"
                unique_dic["message"] = " ".join("Acquisitions is believed to be \
                    anat/inplaneT1 because 'inplaneT1' is in the SeriesDescription. \
                    Please modify if incorrect".split())
            elif "inplanet2" in sd:
                unique_dic["ModalityLabel"] = "inplaneT2"
                unique_dic["message"] = " ".join("Acquisitions is believed to be \
                    anat/inplaneT2 because 'inplaneT2' is in the SeriesDescription.\
                    Please modify if incorrect".split())
            elif "pdt2" in sd:
                unique_dic["ModalityLabel"] = "PDT2"
                unique_dic["message"] = " ".join("Acquisitions is believed to be \
                    anat/PDT2 because 'PDT2' is in the SeriesDescription. \
                    Please modify if incorrect".split())
            elif "pdw" in sd:
                unique_dic["ModalityLabel"] = "PDw"
                unique_dic["message"] = " ".join("Acquisitions is believed to be \
                    anat/PDw because 'PDw' is in the SeriesDescription. \
                    Please modify if incorrect".split())


        # # anatomical parametric maps
        #   elif any(x in sd for x in anat_parametric_keys):
        #       unique_dic["DataType"] = "anat"
        #       if "chimap" in sd:
        #           unique_dic["ModalityLabel"] = "Chimap"
        #           unique_dic["message"] = "Acquisitions is believed to be \
        #               anat/Chimap because "Chimap" is in the SeriesDescription. \
        #                   Please modify if incorrect"
        #       elif "t1rho" in sd:
        #           unique_dic["ModalityLabel"] = "T1rho"
        #           unique_dic["message"] = "Acquisitions is believed to be \
        #               anat/T1rho because "T1rho" is in the SeriesDescription. \
        #                   Please modify if incorrect"
        #       elif "mtsat" in sd:
        #           unique_dic["ModalityLabel"] = "MTsat"
        #           unique_dic["message"] = "Acquisitions is believed to be \
        #               anat/MTsat because "MTsat" is in the SeriesDescription. \
        #                   Please modify if incorrect"
        #       else:
        #           modality_label = [x for x in anat_parametric_keys
        #                             if re.findall(x, sd)][0]
        #           unique_dic["ModalityLabel"] = modality_label.split("map")[0].upper() + "map"
        #           unique_dic["message"] = "Acquisition is believed to be \
        #               anat/{} because {} is in the SeriesDescription. Please \
        #                   modify if incorrect".format(modality_label, modality_label)


        # Can"t discern from SeriesDescription, try using ndim and number of
        # volumes to see if this is a func/bold.
        else:
            test = unique_dic['nibabel_image']
            if test.ndim == 4 and test.shape[3] >= 50 and not any(x in unique_dic["ImageType"] for x in ["DERIVED", "PERFUSION", "DIFFUSION", "ASL"]):
                unique_dic["DataType"] = "func"
                unique_dic["ModalityLabel"] = "bold"
                unique_dic["message"] = " ".join("SeriesDescription did not provide \
                    hints regarding the type of acquisition; however, it is \
                    believed to be a func/bold because it contains >= 50 \
                    volumes and is 4D. Please modify if incorrect".split())

            # Assume not BIDS-compliant acquisition unless user specifies so
            else:
                unique_dic["error"] = " ".join("Acquisition cannot be resolved. Please \
                    determine whether or not this acquisition should be \
                    converted to BIDS".split())
                unique_dic["message"] = " ".join("Acquisition is unknown because there \
                    is not enough adequate information, primarily in the \
                    SeriesDescription. Please modify if acquisition is desired \
                    for BIDS conversion, otherwise the acqusition will not be \
                    converted".split())
                unique_dic["br_type"] = "exclude"


        # Combine DataType and ModalityLabel to create br_type variable, which
        # is needed for internal brainlife.io storage.
        if "exclude" not in unique_dic["br_type"]:
            unique_dic["br_type"] = unique_dic["DataType"] + "/" + unique_dic["ModalityLabel"]
        else:
            pass

        # Combine info above into dictionary, which will be displayed to user
        # through the UI.
        series_info = {"SeriesDescription": unique_dic["SeriesDescription"],
                       "series_idx": unique_dic["series_idx"],
                       "EchoTime": unique_dic["EchoTime"],
                       "ImageType": unique_dic["ImageType"],
                       "RepetitionTime": unique_dic["RepetitionTime"],
                       "entities": series_entities,
                       "type": unique_dic["br_type"],
                       "forType": unique_dic["forType"],
                       "error": unique_dic["error"],
                       "message": unique_dic["message"],
                       "object_indices": []}
        unique_series_list.append(series_info)


        print(" ".join("Unique data acquisition file {}, Series Description {}, was \
              determined to be {}".format(unique_dic["nifti_path"], unique_dic["SeriesDescription"], unique_dic["br_type"]).split()))
        print("")
        print("")


    # Set non-normalized (i.e. poor contrast) anatomicals to exclude, but
    # only if the dataset does not include normalized anatomicals.
    anat_list = [[x['type'], x['ImageType']] for x in unique_series_list if x['type'] == "anat/T1w"]
    anat_list = [x for x in anat_list if 'NORM' in x[1] or 'DERIVED' in x[1]]

    for unique in unique_series_list:
        if unique["type"] == "anat/T1w" and not any(x in ["DERIVED", "NORM"] for x in unique["ImageType"]):
            if len(anat_list):
                unique["error"] = " ".join("Acquisition is a poor contrast {} \
                    (non-normalized); Please check to see if this {} acquisition \
                    should be converted to BIDS. Otherwise, this object will not \
                    be included in the BIDS output".format(unique["type"], unique["type"]).split())
                unique["message"] = unique["error"]
                unique["type"] = "exclude"


    """
    Check sbref acquisitions to see that their corresponding func or dwi
    acquisitions are not excluded. If so, exclude the sbref as well. Also
    exclude sbref if they correspond to an anat or fmap file; currently not
    supported by BIDS
    """

    for s_list in unique_series_list:
        if "sbref" in s_list["type"]:
            sd = s_list["SeriesDescription"]
            corresponding_file = [y for x, y in enumerate(unique_series_list)
                                  if sd[:-6] == unique_series_list[x]["SeriesDescription"]][0]
            if corresponding_file["type"] == "exclude":
                s_list["message"] = " ".join("The corresponding file to this sbref is \
                    excluded, therefore this acquisition will also be excluded. \
                    Please modify if incorrect".split())
                s_list["type"] = "exclude"
            if "anat" or "fmap" in corresponding_file["type"]:
                s_list["message"] = " ".join("The corresponding file to this sbref is \
                    an anatomical or field map acquisition. BIDS currently does \
                    not support sbref for these acquisitions, therefore this \
                    sbref will be excluded. Please modify if incorrect".split())


    # If series_entities items contain periods (not allowed in BIDS) then
    # replace them with "p".
    for s_list in unique_series_list:
        for key, value in s_list["entities"].items():
            try:
                if "." in value:
                    s_list["entities"][key] = value.replace(".", "p")
            except:
                pass

    return unique_series_list

def update_dataset_list(dataset_list, unique_series_list):
    """
    Update the dataset_list with information that we found from the unique
    series list. Since the unique_series_list does not contain all dataset
    acquisitions, use the unique series ID (series_idx) to port information
    over.
    """
    for series in unique_series_list:
        for data in dataset_list:
            if data["series_idx"] == series["series_idx"]:
                data["entities"] = series["entities"]
                data["type"] = series["type"]
                data["forType"] = series["forType"]
                data["error"] = series["error"]
                data["message"] = series["message"]

    return dataset_list

def modify_objects_info(dataset_list):
    """
    Make any necessary changes to the objects level.

    Parameters
    ----------
    dataset_list: list
        List of dictionaries containing pertinent and unique information about
        the data, primarily coming from the metadata in the json files

    Returns
    -------
    scan_protocol: list
        Same as above but with updated information
    """
    objects_list = []

    # Find unique subject/session pairs in dataset and sort them
    subj_ses_pairs = [[x["subject"], x["session"]] for x in dataset_list]
    unique_subj_ses_pairs = sorted([list(i) for i in set(tuple(i) for i in subj_ses_pairs)])

    for unique_subj_ses in unique_subj_ses_pairs:
        scan_protocol = [x for x in dataset_list
                         if x["subject"] == unique_subj_ses[0]
                         and x["session"] == unique_subj_ses[1]]

        # sort scan protocol
        scan_protocol = sorted(scan_protocol, key=itemgetter("AcquisitionTime",
                                                    "series_idx"))

        section_id = 1
        objects_data = []

        # Peruse scan protocol to check for potential issues and add some
        # additional information.

        for p, protocol in enumerate(scan_protocol):
            previous_message = scan_protocol[p-1]["message"]

            # Update section_id information
            if p == 0:
                protocol["section_ID"] = section_id
            elif "localizer" in protocol["message"] and "localizer" not in previous_message:
                section_id += 1
                protocol["section_ID"] = section_id
            else:
                protocol["section_ID"] = section_id

            image = protocol['nibabel_image']
            protocol["headers"] = str(image.header).splitlines()[1:]


            object_img_array = image.dataobj
            if object_img_array.dtype not in ["<i2", "<u2"]:
                # Weird issue where data array is RGB instead of intger
                protocol["exclude"] = True
                protocol["error"] = "The data array is for this acquisition is \
                    improper, likely suggesting some issue with the corresponding \
                    DICOMS"
                protocol["message"] = protocol["error"]
                protocol["br_type"] = "exclude"
            else:
                # Generate screenshot of every acquisition in dataset
                create_screenshots(protocol['nifti_path'], image, protocol["NumVolumes"])


            if protocol["error"]:
                protocol["error"] = [protocol["error"]]
            else:
                protocol["error"] = []

            objects_entities = {"subject": "",
                                "session": "",
                                "run": "",
                                "task": "",
                                "direction": "",
                                "acquisition": "",
                                "ceagent": "",
                                "echo": "",
                                "fa": "",
                                "inversion": "",
                                "part": ""}

            # Make items list (part of objects list)
            items = []
            for item in protocol["paths"]:
                if ".bval" in item:
                    items.append({"path":item,
                                  "name":"bval"})
                elif ".bvec" in item:
                    items.append({"path":item,
                                  "name":"bvec"})
                elif ".json" in item:
                    items.append({"path":item,
                                  "name":"json",
                                  "sidecar":protocol["sidecar"]})
                elif ".nii.gz" in item:
                    items.append({"path":item,
                                  "name":"nii.gz",
                                  "headers":protocol["headers"]})

            # Remove identifying information from sidecars.
            # Same as dcm2niix -ba y
            remove_fields = ["SeriesInstanceUID",
                             "StudyInstanceUID",
                             "ReferringPhysicianName",
                             "StudyID",
                             "PatientName",
                             "PatientID",
                             "AccessionNumber",
                             "PatientBirthDate",
                             "PatientSex",
                             "PatientWeight"]

            for remove in remove_fields:
                if remove in protocol["sidecar"]:
                    del protocol["sidecar"][remove]

            # Provide log output for acquisitions not deemed appropriate for
            # BIDS conversion.
            if protocol["exclude"]:
                print("")
                print("* {} (sn-{}) not recommended for BIDS conversion: {}".format(protocol["SeriesDescription"], protocol["SeriesNumber"], protocol["error"]))

            # Objects-level info for ezBIDS.json
            objects_info = {"series_idx": protocol["series_idx"],
                            "PatientName": protocol["PatientName"],
                            "PatientID": protocol["PatientID"],
                            "PatientBirthDate": protocol["PatientBirthDate"],
                            "AcquisitionDate": protocol["AcquisitionDate"],
                            "pngPath": "{}.png".format(protocol["nifti_path"][:-7]),
                            "entities": objects_entities,
                            "items": items,
                            "analysisResults": {
                                "NumVolumes": protocol["NumVolumes"],
                                "errors": protocol["error"],
                                "filesize": protocol["filesize"],
                                "section_ID": protocol["section_ID"]},
                            "paths": protocol["paths"]}
            objects_data.append(objects_info)


        objects_list.append(objects_data)

    # Flatten list of lists
    objects_list = [x for y in objects_list for x in y]

    return objects_list


##################### Begin #####################

print("########################################")
print("Beginning conversion process of uploaded dataset")
print("########################################")
print("")

# participantsColumn portion of ezBIDS.json
PARTICIPANTS_COLUMN = {"sex": {"LongName": "gender",
                               "Description": "generic gender field",
                               "Levels": {
                                   "M": "male",
                                   "F": "female"
                                   }
                               },
                       "age": {"LongName": "age",
                               "Units": "years"
                               }
                       }

uploaded_json_list = pd.read_csv("list", header=None, sep="\n").to_numpy().flatten().tolist()

uploaded_files_list = modify_uploaded_dataset_list(uploaded_json_list)

dataset_list = generate_dataset_list(uploaded_files_list)

dataset_list, subject_ids_info = determine_subj_ses_IDs(dataset_list)

dataset_list, dataset_list_unique_series = determine_unique_series(dataset_list)

unique_series_list = identify_series_info(dataset_list_unique_series)

dataset_list = update_dataset_list(dataset_list, unique_series_list)

objects_list = modify_objects_info(dataset_list)


for s in range(len(unique_series_list)):
    unique_series_list[s]["object_indices"] = [x for x in range(len(objects_list)) if objects_list[x]["series_idx"] == unique_series_list[s]["series_idx"]]

# Convert infor to dictionary
EZBIDS = {"subjects": subject_ids_info,
          "participantsColumn": PARTICIPANTS_COLUMN,
          "series": unique_series_list,
          "objects": objects_list
          }

# Write dictionary to ezBIDS.json
with open("ezBIDS.json", "w") as fp:
    json.dump(EZBIDS, fp, indent=3)